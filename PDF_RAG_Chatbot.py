# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import streamlit as st
import os
import tempfile
# from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from operator import itemgetter
from typing import List, Dict, Any

# 2. í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF RAG ì±—ë´‡ (LCEL)",
    page_icon="ğŸ“š",
    layout="wide"
)

# 3. í™˜ê²½ ë³€ìˆ˜ ë° API í‚¤ ì„¤ì •
#@st.cache_data
#def load_environment():
    """í™˜ê²½ë³€ìˆ˜ ë¡œë“œ"""
#    load_dotenv()
#    return os.getenv("OPENAI_API_KEY")

# 4. PDF ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
@st.cache_data
def load_pdf(file_path):
    """PDF ë¡œë“œ ë° ë¶„í• """
    try:
        loader = PyPDFLoader(file_path)
        data = loader.load_and_split()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        documents = text_splitter.split_documents(data)
        
        return documents, len(data)
    except Exception as e:
        st.error(f"PDF ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, 0

@st.cache_resource
def create_vectorstore(documents, api_key):
    """ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    try:
        embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            api_key=api_key
        )
        vectorstore = FAISS.from_documents(documents, embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# 5. LCEL ê¸°ë°˜ ëŒ€í™” ì²´ì¸ ìƒì„±
def create_lcel_conversation_chain(vectorstore, api_key):
    """LCELì„ ì‚¬ìš©í•œ ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
    try:
        # LLM ì´ˆê¸°í™”
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            api_key=api_key,
            temperature=0
        )
        
        # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )
        
        # ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ê³ ë ¤)
        contextualize_q_system_prompt = (
            "ì£¼ì–´ì§„ ì±„íŒ… ê¸°ë¡ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, "
            "ì±„íŒ… ê¸°ë¡ì˜ ë§¥ë½ì„ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ "
            "ì±„íŒ… ê¸°ë¡ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”. "
            "ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì¬êµ¬ì„±í•˜ê³  "
            "ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."
        )
        
        contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])
        
        # ì§ˆë¬¸ ì¬êµ¬ì„± ì²´ì¸
        history_aware_retriever = contextualize_q_prompt | llm | StrOutputParser() | retriever
        
        # QA í”„ë¡¬í”„íŠ¸
        qa_system_prompt = (
            "ë‹¹ì‹ ì€ ì§ˆë¬¸ ë‹µë³€ ì‘ì—…ì„ ìœ„í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
            "ë‹¤ìŒì˜ ê²€ìƒ‰ëœ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. "
            "ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. "
            "ë‹µë³€ì€ ìµœëŒ€ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. "
            "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
            "{context}"
        )
        
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])
        
        # ë¬¸ì„œ ê²°í•© ì²´ì¸
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        
        # ìµœì¢… RAG ì²´ì¸
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        
        return rag_chain
        
    except Exception as e:
        st.error(f"LCEL ì²´ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# 6. ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def format_chat_history(chat_history: List[tuple]) -> List:
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    formatted_history = []
    for human_msg, ai_msg in chat_history:
        formatted_history.append(HumanMessage(content=human_msg))
        formatted_history.append(AIMessage(content=ai_msg))
    return formatted_history

def get_session_history() -> List:
    """ì„¸ì…˜ì—ì„œ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    return format_chat_history(st.session_state.chat_history)

# 7. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    st.title("ğŸ“š PDF RAG ì±—ë´‡ (LCEL ë²„ì „)")
    st.markdown("**LCEL(LangChain Expression Language)**ì„ í™œìš©í•œ í˜„ëŒ€ì  RAG ì±—ë´‡")
    
    # API í‚¤ í™•ì¸
    api_key = load_environment()
    if not api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        api_key = st.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
        if not api_key:
            st.stop()
    
    # ì‚¬ì´ë“œë°” - íŒŒì¼ ì—…ë¡œë“œ ë° ì„¤ì •
    with st.sidebar:
        st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader(
            "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type="pdf",
            help="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )
        
        use_default = st.checkbox("ê¸°ë³¸ íŒŒì¼ ì‚¬ìš© (ì†Œë‚˜ê¸°.pdf)")
        
        if st.button("ğŸ”„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘"):
            if uploaded_file or use_default:
                with st.spinner("ğŸ“– ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
                    if use_default:
                        file_path = './data/ì†Œë‚˜ê¸°.pdf'
                        if not os.path.exists(file_path):
                            st.error("ê¸°ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                            return
                    else:
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                            tmp_file.write(uploaded_file.getvalue())
                            file_path = tmp_file.name
                    
                    # ë¬¸ì„œ ì²˜ë¦¬
                    documents, page_count = load_pdf(file_path)
                    
                    if documents:
                        st.session_state.documents = documents
                        st.session_state.page_count = page_count
                        
                        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                        vectorstore = create_vectorstore(documents, api_key)
                        
                        if vectorstore:
                            st.session_state.vectorstore = vectorstore
                            
                            # LCEL ì²´ì¸ ìƒì„±
                            rag_chain = create_lcel_conversation_chain(vectorstore, api_key)
                            
                            if rag_chain:
                                st.session_state.rag_chain = rag_chain
                                st.success(f"âœ… LCEL ì²´ì¸ ìƒì„± ì™„ë£Œ! ({page_count}ê°œ í˜ì´ì§€, {len(documents)}ê°œ ì²­í¬)")
                    
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    if not use_default and os.path.exists(file_path):
                        os.unlink(file_path)
            else:
                st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        # ì²´ì¸ ì •ë³´ í‘œì‹œ
        if 'rag_chain' in st.session_state:
            st.subheader("ğŸ”— ì²´ì¸ ì •ë³´")
            st.success("LCEL RAG ì²´ì¸ í™œì„±í™”")
            
            with st.expander("ğŸ“‹ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­"):
                st.markdown("""
                **ì‚¬ìš©ëœ LCEL ì»´í¬ë„ŒíŠ¸:**
                - `ChatPromptTemplate`: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
                - `create_retrieval_chain`: ê²€ìƒ‰ ì²´ì¸
                - `create_stuff_documents_chain`: ë¬¸ì„œ ê²°í•© ì²´ì¸
                - `MessagesPlaceholder`: ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬
                - History-aware retrieverë¡œ ëŒ€í™” ë§¥ë½ ê³ ë ¤
                """)
    
    # ë©”ì¸ ì˜ì—­ - ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    if 'rag_chain' in st.session_state:
        st.header("ğŸ’¬ LCEL ê¸°ë°˜ ì±„íŒ…")
        
        # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
        if 'chat_history' not in st.session_state:
            st.session_state.chat_history = []
        
        # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
        chat_container = st.container()
        with chat_container:
            for i, (question, answer) in enumerate(st.session_state.chat_history):
                st.markdown(f"**ğŸ§‘â€ğŸ’» ì§ˆë¬¸:** {question}")
                st.markdown(f"**ğŸ¤– ë‹µë³€:** {answer}")
                st.divider()
        
        # ì§ˆë¬¸ ì…ë ¥
        question = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: ì†Œë…„ì€ ì–´ë””ì—ì„œ ì²˜ìŒ ì†Œë…€ë¥¼ ë§Œë‚¬ë‚˜ìš”?",
            key="question_input"
        )
        
        # ì§ˆë¬¸ ì²˜ë¦¬ (LCEL ì²´ì¸ ì‚¬ìš©)
        if st.button("ğŸ“¤ ì§ˆë¬¸í•˜ê¸°") and question:
            with st.spinner("ğŸ¤” LCEL ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    # í˜„ì¬ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
                    chat_history = get_session_history()
                    
                    # LCEL ì²´ì¸ ì‹¤í–‰
                    result = st.session_state.rag_chain.invoke({
                        "input": question,
                        "chat_history": chat_history
                    })
                    
                    answer = result["answer"]
                    
                    # ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
                    st.session_state.chat_history.append((question, answer))
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ í‘œì‹œ (ì„ íƒì‚¬í•­)
                    if st.checkbox("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ë³´ê¸°"):
                        with st.expander("ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤"):
                            for i, doc in enumerate(result.get("context", [])):
                                st.write(f"**ë¬¸ì„œ {i+1}:**")
                                st.write(doc.page_content[:300] + "...")
                                st.divider()
                    
                    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"LCEL ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
        if st.session_state.chat_history:
            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"):
                    st.session_state.chat_history = []
                    st.rerun()
            with col2:
                if st.button("ğŸ’¾ ëŒ€í™” ë‚´ë³´ë‚´ê¸°"):
                    conversation_text = ""
                    for q, a in st.session_state.chat_history:
                        conversation_text += f"Q: {q}\nA: {a}\n\n"
                    
                    st.download_button(
                        label="ğŸ“¥ ëŒ€í™” ë‹¤ìš´ë¡œë“œ",
                        data=conversation_text,
                        file_name="conversation_history.txt",
                        mime="text/plain"
                    )
        
        # ë¬¸ì„œ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“Š ë¬¸ì„œ ë° ì²´ì¸ ì •ë³´"):
            col1, col2 = st.columns(2)
            with col1:
                st.metric("í˜ì´ì§€ ìˆ˜", st.session_state.page_count)
                st.metric("ì²­í¬ ìˆ˜", len(st.session_state.documents))
            with col2:
                st.metric("ëŒ€í™” ìˆ˜", len(st.session_state.chat_history))
                st.write("**ì²´ì¸ íƒ€ì…:** LCEL RAG Chain")
    
    else:
        st.info("ğŸ‘† ì‚¬ì´ë“œë°”ì—ì„œ PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
        
        # LCEL ì¥ì  ì„¤ëª…
        st.subheader("ğŸš€ LCELì˜ ì¥ì ")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **ğŸ”§ ê¸°ìˆ ì  ê°œì„ ì‚¬í•­:**
            - ë” ëª…í™•í•œ ì²´ì¸ êµ¬ì¡°
            - ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
            - ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
            - ë” ë‚˜ì€ ì˜¤ë¥˜ ì²˜ë¦¬
            """)
        
        with col2:
            st.markdown("""
            **ğŸ’¡ ê¸°ëŠ¥ì  ê°œì„ ì‚¬í•­:**
            - ëŒ€í™” ë§¥ë½ ì¸ì‹
            - ì§ˆë¬¸ ì¬êµ¬ì„± ê¸°ëŠ¥
            - ê²€ìƒ‰ ë¬¸ì„œ ë°˜í™˜
            - ëª¨ë“ˆí™”ëœ êµ¬ì¡°
            """)
        
        # ì˜ˆì‹œ ì§ˆë¬¸ í‘œì‹œ
        st.subheader("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸ë“¤")
        example_questions = [
            "ì†Œë…„ì€ ì–´ë””ì—ì„œ ì²˜ìŒ ì†Œë…€ë¥¼ ë§Œë‚¬ë‚˜?",
            "ì†Œë…„ì€ ì†Œë…€ì—ê²Œ ë¬´ì—‡ì„ ì£¼ë ¤ê³  í–ˆë‚˜?", 
            "ê·¸ ì¥ì†ŒëŠ” ì–´ë–¤ íŠ¹ì§•ì´ ìˆì—ˆë‚˜?",
            "ì†Œë‚˜ê¸°ëŠ” ì–¸ì œ ë‚´ë ¸ë‚˜?"
        ]
        
        for q in example_questions:
            st.markdown(f"â€¢ {q}")

# 8. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
if __name__ == "__main__":
    main()
